# Features for large datasets - optimize for speed & memory
ingestion:
  low_memory: true
  chunksize: 100000
  max_file_size_mb: 50000

validation:
  enable: true
  null_threshold: 0.98
  duplicate_threshold: 0.80

splitting:
  strategy: random
  test_size: 0.15
  val_size: 0.05
  shuffle: true
  random_state: 42

profiling:
  enable: false   # Too slow for large datasets
